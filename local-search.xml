<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>位置编码</title>
    <link href="/2023/12/09/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"/>
    <url>/2023/12/09/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>分词</title>
    <link href="/2023/12/09/%E5%88%86%E8%AF%8D/"/>
    <url>/2023/12/09/%E5%88%86%E8%AF%8D/</url>
    
    <content type="html"><![CDATA[<p>我们要将字符映射成计算机能够识别的数字必须要经历分词这一步。本文主要讲解英文分析的主要步骤。</p><h2 id="1-char-subwords-word">1. char-<strong>subwords</strong>-word</h2><p>很自然的我们对每一个独立含义的英文单词独立编码，即apple. dog, chicken 他们分别拥有独立的编码，也是word级别的编码，word级别的编码可以让每个单词都有固定的符号，但是面对一个问题就是英语词库中有太多的英语单词，需要编码的量远超过计算机的运算能力。同时随着互联网的发展，新词还在不断涌现，word编码陷入窘迫。<br>Char编码应运而生。char编码是只对26个字母进行编码，所以可以很轻松的囊括所有现存的以及将要发展的英文语系下的单词。但是对比于word编码char编码本身没有任何含义，也就是a，b等这些没有任何含义。单纯的表达能力不强。而且char中每个字母蕴含太多语义优化起来比较困难。<br>介于两者之间的Subwords分词出现了。也就是子词词元化。它能够平衡词表大小与容纳的语义。并就如何进行子词词元化产生了很多方法。</p><h2 id="2-subwords">2. subwords</h2><h3 id="字节对编码-Byte-Pair-Encoding-BPE">字节对编码(<a href="https://arxiv.org/abs/1508.07909">Byte-Pair Encoding</a>, BPE):</h3><p><strong>GPT-2, GPT-3,LLAMA-1, LLAMA-2, Roberta, GPT-3.5?, GPT-4?</strong></p><ol><li>提取所有的单词并计算单词出现的次数</li><li>所有的单词拆分成字符</li><li>确定词表大小</li><li>将非重复字母添加到词表中</li><li>检查词表中出现<strong>最频繁</strong>的符号对，将其加入词表。加入此表的的符号对以一个单独的词继续参与此迭代。</li><li>词表大小达到要求，停止统计</li></ol><h3 id="字节级字节对编码-Byte-level-BPE-BBPE">字节级字节对编码(<a href="">Byte-level BPE</a>, BBPE)</h3><h3 id="WordPiece编码"><a href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf">WordPiece</a>编码</h3><p>** BERT, DistilBERT, Electra**<br>通过公式：<br>$$p(st)/p(s)p(t)$$<br>计算符号对s和t的<strong>相似度</strong>来决定添加的单词对，而BPE是计算符号对的频率。</p><h3 id="N-gram编码"><a href="https://arxiv.org/pdf/1804.10959.pdf">N-gram</a>编码</h3><h3 id="sentence-piece">[sentence-piece]</h3><h2 id="OOV问题">OOV问题</h2><p>进行非字符级别编码面对的问题就是OOV问题，所给词未在词表中的问题。一般用[UNK]表示</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Bert-变体</title>
    <link href="/2023/12/09/Bert-%E5%8F%98%E4%BD%93/"/>
    <url>/2023/12/09/Bert-%E5%8F%98%E4%BD%93/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Bert-下游微调</title>
    <link href="/2023/12/09/Bert-%E4%B8%8B%E6%B8%B8%E5%BE%AE%E8%B0%83/"/>
    <url>/2023/12/09/Bert-%E4%B8%8B%E6%B8%B8%E5%BE%AE%E8%B0%83/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Bert-基本构成</title>
    <link href="/2023/12/09/Bert-%E5%9F%BA%E6%9C%AC%E6%9E%84%E6%88%90/"/>
    <url>/2023/12/09/Bert-%E5%9F%BA%E6%9C%AC%E6%9E%84%E6%88%90/</url>
    
    <content type="html"><![CDATA[<p>Bert将Transformer左半边的Encoder拿出来进行单独训练，来生成预训练模型。GPT是将Transformers右半边Decoder拿出来进行预训练。整体的框架如下：<br><img src="../images/bert/bert_framworks.jpg" alt="bert-framewoek"></p><h2 id="词嵌入：">词嵌入：</h2><p>词嵌入Bert选择了WordPiece进行子词词元化。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>语言的起源</title>
    <link href="/2023/12/01/%E8%AF%AD%E8%A8%80%E7%9A%84%E8%B5%B7%E6%BA%90/"/>
    <url>/2023/12/01/%E8%AF%AD%E8%A8%80%E7%9A%84%E8%B5%B7%E6%BA%90/</url>
    
    <content type="html"><![CDATA[<h2 id="起源">起源</h2><p>语言并不是通过有一个先知的上帝，发明或者创造了语言，而是一种共识。现如今的语言学家更多是发现了共识背后可以书写的内容，但是语言的诞生往往比规则的诞生早非常久。所以现如今网络流行语一直在更换，但最基本，最被人们认可的共识，并未被人们抛弃。</p><p>但是我们作为人各个物种，我们有相同的生理需求，同时又相同的恐惧感受方式，再一片地区我们有相同的思维模式，这些共同之处也意味着语言背后有我们认可的一项规则。</p><p>上面两段模糊的描述是为了说明，对待语言我们在某种模式下行走不要钻进死胡同，同时也不可对这些规则不屑一顾，更多要去秉承一颗使用主义的心。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>预训练模型整体流程</title>
    <link href="/2023/12/01/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B/"/>
    <url>/2023/12/01/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<pre class="mermaid">graph TB;A(载入预训练模型) --> B(重载模型);B --加入--> C(学习率);B --加入--> D(优化器);B --加入--> E(损失函数);C --> F(初始化热身);D --> F(初始化热身);E --> F(初始化热身);F --> G(热身结束，更新参数);subgraph 每个epoch ;   G --> H(计算损失);   H --更新--> H;end;H --> I(结束);</pre>]]></content>
    
    
    
    <tags>
      
      <tag>工程实践</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据迭代器</title>
    <link href="/2023/12/01/%E6%95%B0%E6%8D%AE%E8%BF%AD%E4%BB%A3%E5%99%A8/"/>
    <url>/2023/12/01/%E6%95%B0%E6%8D%AE%E8%BF%AD%E4%BB%A3%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<pre class="mermaid">classDiagramClass01 <|-- AveryLongClass : CoolClass03 *-- Class04Class05 o-- Class06Class07 .. Class08Class09 --> C2 : Where am i?Class09 --* C3Class09 --|> Class07Class07 : equals()Class07 : Object[] elementDataClass01 : size()Class01 : int chimpClass01 : int gorillaClass08 <--> C2: Cool label</pre>]]></content>
    
    
    
    <tags>
      
      <tag>数据</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据预处理</title>
    <link href="/2023/12/01/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <url>/2023/12/01/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h2 id="1-数据格式">1. 数据格式</h2><h3 id="JSON">JSON</h3><p>数据优先转换成字典，之后转化成json。<br>对于相同格式的多个数据，转化成列表中的字典。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>数据的流动</title>
    <link href="/2023/11/30/%E6%95%B0%E6%8D%AE%E7%9A%84%E6%B5%81%E5%8A%A8/"/>
    <url>/2023/11/30/%E6%95%B0%E6%8D%AE%E7%9A%84%E6%B5%81%E5%8A%A8/</url>
    
    <content type="html"><![CDATA[<p>电子设备中流动的抽象的0或者1的离散信号，如果再仔细看大多数是电子的有无通断。<br>计算机现如今只能识别以及处理0、1信号，我们的所有行动都以此为基础，并展开。</p><h2 id="自然语言">自然语言</h2><p>自然界中的语言大多分两种表音或者形意，行意的文字历史最早出现，现如今还被人继续使用的有且仅有汉字这一种。<br>其他的世界领域都被表音文字所占领。<br>表音文字易于传播，但存储以及一致性交叉，所以现如今的最流行的英语500年前的英语，除非是这个领域的研究人员，大多数都不知道怎么读。但是表意文字作为信息的强载体，受过义务教育的人也能勉强看懂2000年前的汉字。但是这种强的信息载体，阻碍了寻迅速传播，现如今已经只有1/7的地球人在使用这种文字。<br>在0、1的抽象结构中，我们试着沟通自然界与抽象的0、1之间。将01组合起来便得到二进制的数字，抽象的数字是必定可以表示数字的，也就是我们所说的43在电脑中是能够表示出来的，但是对于字母“ABC”呢？表达方式自然不一样，在计算机中最初就诞生了一开始流行的标准ASCII，把我们现如今键盘上用到的字符进行编码进行存储与展示:</p><table><thead><tr><th>字符</th><th>二进制</th><th>数字</th><th>十六进制</th><th>Unicode</th></tr></thead><tbody><tr><td>A</td><td>0100 0001</td><td>65</td><td>0x41</td><td>U+0041</td></tr></tbody></table><p>ASCII出现的时期决定了它本身承载的容量小，以及语言被集中在英语语系。因特网让地球村联系到了一起，不同的语言连接到一起，自然孕育了更大的，现如今更通用的Unicode编码，这个编码收录了世界上官方流行的语言如：简体、繁体、西班牙语、藏语等，方便了人们进行不同地区的编码，他们编码后能在一个平台编辑、浏览。</p><h2 id="机器语言">机器语言</h2><h2 id="问题与反向">问题与反向</h2><p>自然语言的特征决定了它不能够进行简单的输入输出，需要携带冗余的信息来适配现如今的抽象规则。<br>不仅仅是语言转化为数字这么简单，同时还需要将语言位置进行携带，如果是进行机器学习的训练的话，但涉及到证据提供我们还需要携带位置的信息，开始的位置与结束的位置，对于不等长的数据我们同步传递进去的还有哪些不等长的开始的位置以及结束的位置。我们不得不在信息载体上面进行拖妥协来换取并行计算的优势。</p>]]></content>
    
    
    
    <tags>
      
      <tag>thinking</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
