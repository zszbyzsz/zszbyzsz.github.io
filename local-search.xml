<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>位置编码</title>
    <link href="/2023/12/09/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"/>
    <url>/2023/12/09/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>分词</title>
    <link href="/2023/12/09/%E5%88%86%E8%AF%8D/"/>
    <url>/2023/12/09/%E5%88%86%E8%AF%8D/</url>
    
    <content type="html"><![CDATA[<p>我们要将字符映射成计算机能够识别的数字必须要经历分词这一步。本文主要讲解英文分析的主要步骤。</p><h2 id="1-char-subwords-word">1. char-<strong>subwords</strong>-word</h2><p>很自然的我们对每一个独立含义的英文单词独立编码，即apple. dog, chicken 他们分别拥有独立的编码，也是word级别的编码，word级别的编码可以让每个单词都有固定的符号，但是面对一个问题就是英语词库中有太多的英语单词，需要编码的量远超过计算机的运算能力。同时随着互联网的发展，新词还在不断涌现，word编码陷入窘迫。<br>Char编码应运而生。char编码是只对26个字母进行编码，所以可以很轻松的囊括所有现存的以及将要发展的英文语系下的单词。但是对比于word编码char编码本身没有任何含义，也就是a，b等这些没有任何含义。单纯的表达能力不强。而且char中每个字母蕴含太多语义优化起来比较困难。<br>介于两者之间的Subwords分词出现了。也就是子词词元化。它能够平衡词表大小与容纳的语义。并就如何进行子词词元化产生了很多方法。</p><h2 id="2-subwords">2. subwords</h2><h3 id="字节对编码-Byte-Pair-Encoding-BPE">字节对编码(<a href="https://arxiv.org/abs/1508.07909">Byte-Pair Encoding</a>, BPE):</h3><p><strong>GPT-2, GPT-3,LLAMA-1, LLAMA-2, Roberta, GPT-3.5?, GPT-4?</strong></p><ol><li>提取所有的单词并计算单词出现的次数</li><li>所有的单词拆分成字符</li><li>确定词表大小</li><li>将非重复字母添加到词表中</li><li>检查词表中出现<strong>最频繁</strong>的符号对，将其加入词表。加入此表的的符号对以一个单独的词继续参与此迭代。</li><li>词表大小达到要求，停止统计</li></ol><h3 id="字节级字节对编码-Byte-level-BPE-BBPE">字节级字节对编码(<a href="">Byte-level BPE</a>, BBPE)</h3><h3 id="WordPiece编码"><a href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf">WordPiece</a>编码</h3><p><strong>BERT, DistilBERT, Electra</strong><br>通过公式：<br>$$p(st)/p(s)p(t)$$<br>计算符号对s和t的<strong>相似度</strong>来决定添加的单词对，而BPE是计算符号对的频率。</p><h3 id="N-gram编码"><a href="https://arxiv.org/pdf/1804.10959.pdf">N-gram</a>编码</h3><h3 id="sentence-piece">[sentence-piece]</h3><h2 id="OOV问题">OOV问题</h2><p>进行非字符级别编码面对的问题就是OOV问题，所给词未在词表中的问题。一般用[UNK]表示</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Bert-变体</title>
    <link href="/2023/12/09/Bert-%E5%8F%98%E4%BD%93/"/>
    <url>/2023/12/09/Bert-%E5%8F%98%E4%BD%93/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
    <tags>
      
      <tag>BERT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Bert-下游微调</title>
    <link href="/2023/12/09/Bert-%E4%B8%8B%E6%B8%B8%E5%BE%AE%E8%B0%83/"/>
    <url>/2023/12/09/Bert-%E4%B8%8B%E6%B8%B8%E5%BE%AE%E8%B0%83/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
    <tags>
      
      <tag>BERT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Bert-基本构成</title>
    <link href="/2023/12/09/Bert-%E5%9F%BA%E6%9C%AC%E6%9E%84%E6%88%90/"/>
    <url>/2023/12/09/Bert-%E5%9F%BA%E6%9C%AC%E6%9E%84%E6%88%90/</url>
    
    <content type="html"><![CDATA[<p>Bert将Transformer左半边的Encoder拿出来进行单独训练，来生成预训练模型。GPT是将Transformers右半边Decoder拿出来进行预训练。整体的框架如下：<br><img src="../images/bert/bert_framworks.jpg" alt="bert-framewoek"></p><h2 id="词嵌入：">词嵌入：</h2><p>词嵌入Bert选择了WordPiece进行子词词元化。</p>]]></content>
    
    
    
    <tags>
      
      <tag>BERT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>预训练模型整体流程</title>
    <link href="/2023/12/01/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B/"/>
    <url>/2023/12/01/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<pre class="mermaid">graph TB;A(载入预训练模型) --> B(重载模型)B --加入--> C(学习率)B --加入--> D(优化器)B --加入--> E(损失函数)C --> F(初始化热身)D --> F(初始化热身)E --> F(初始化热身)F --> G(训练)subgraph 每个epoch    G --> H(计算损失)   H --更新--> HendH --一定周期或条件--> Z(结束)</pre>]]></content>
    
    
    
    <tags>
      
      <tag>工程实践</tag>
      
      <tag>模型训练</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
